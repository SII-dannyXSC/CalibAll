<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Stable Offline Hand-Eye Calibration for any Robot with Just One Mark">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="It is a simple yet effective method for camera extrinsic estimation that requires only a single mark. The method follows a coarse-to-fine calibration pipeline, achieving training-free, stable, and accurate results across diverse datasets and robot platforms.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Hand-Eye Calibration, Dataset Enhancement, Imitation Learning, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Sicheng Xie, Lingchen Meng, Zhiying Du, Shuyuan Tu, Haidong Cao, Jiaqi Leng, Zuxuan Wu, Yu-Gang Jiang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Fudan FVL">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Stable Offline Hand-Eye Calibration for any Robot with Just One Mark">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="It is a simple yet effective method for camera extrinsic estimation that requires only a single mark. The method follows a coarse-to-fine calibration pipeline, achieving training-free, stable, and accurate results across diverse datasets and robot platforms.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://sii-dannyxsc.github.io/CalibAll-web/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Stable Offline Hand-Eye Calibration for any Robot with Just One Mark">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Stable Offline Hand-Eye Calibration for any Robot with Just One Mark">
  <meta name="citation_author" content="Sicheng Xie">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Stable Offline Hand-Eye Calibration for any Robot with Just One Mark - Sicheng Xie</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Stable Offline Hand-Eye Calibration for any Robot with Just One Mark",
    "description": "It is a simple yet effective method for camera extrinsic estimation that requires only a single mark. The method follows a coarse-to-fine calibration pipeline, achieving training-free, stable, and accurate results across diverse datasets and robot platforms.",
    "author": [
      {
        "@type": "Person",
        "name": "Sicheng Xie",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University"
        }
      },
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Stable Offline Hand-Eye Calibration for any Robot with Just One Mark</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <!-- Lingchen Meng Zhiying Du Shuyuan Tu Haidong Cao Jiaqi Leng Zuxuan Wu* Yu-Gang Jiang -->
              <span class="author-block">
                <a target="_blank">Sicheng Xie</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a target="_blank">Lingchen Meng</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a target="_blank">Zhiying Du</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a target="_blank">Shuyuan Tu</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a target="_blank">Haidong Cao</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a target="_blank">Jiaqi Leng</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a target="_blank">Zuxuan Wu</a><sup>1,2</sup><sup>*</sup>,
                  </span>
                  <span class="author-block">
                    <a target="_blank">Yu-Gang Jiang</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">
                      <sup>1</sup>Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University
                    <!-- <br>Conference name and year -->
                    </span>
                    <span class="author-block">
                      <sup>2</sup>Shanghai Innovation Institute
                    <!-- <br>Conference name and year -->
                    </span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/SII-dannyXSC/CalibAll" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- Teaser image (replaced video with image) -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- TODO: Replace with your teaser image(s) -->
      <figure class="teaser-image">
        <img src="static/images/method.jpg" alt="Method" loading="lazy" style="width:100%; height:auto; border-radius:6px;"/>
        <figcaption class="has-text-centered" style="margin-top:8px; color:#4a5568;">Architecture overview of CalibAll.</figcaption>
      </figure>

      <!-- 保留原来的视频描述位置，改为图片说明 -->
      <h2 class="subtitle has-text-centered" style="margin-top:18px;">
        It is a simple yet effective method for camera extrinsic estimation that <strong>requires only a single mark</strong>. The method follows a coarse-to-fine calibration pipeline, achieving training-free, stable, and accurate results across diverse datasets and robot platforms.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Imitation learning has achieved remarkable success in a variety of robotic tasks by learning a mapping function from camera-space observations to robot-space actions. Recent work indicates that the use of robot-to-camera transformation information (i.e., camera extrinsics) benefits the learning process and produces better results. However, camera extrinsics are oftentimes unavailable and estimation methods usually suffer from local minima and poor generalizations. 
            In this paper, we present CalibAll, a simple yet effective method that <strong>requires only a single mark</strong> and performs training-free, stable, and accurate camera extrinsic estimation across diverse robots and datasets through a coarse-to-fine calibration pipeline. In particular, we annotate a single mark on an end-effector (EEF), and leverage the correspondence ability emerged from vision foundation models (VFM) to automatically localize the corresponding mark across robots in diverse datasets. Using this mark, together with point tracking and the 3D EEF trajectory, we obtain a coarse camera extrinsic via temporal Perspective-n-Point (PnP). This estimate is further refined through a rendering-based optimization that aligns rendered and ground-true masks,  yielding accurate and stable camera extrinsic. Experimental results demonstrate that our method outperforms state-of-the-art approaches, showing strong robustness and general effectiveness across three robot platforms.  It also produces useful auxiliary annotations such as depth maps, link-wise masks, and end-effector 2D trajectories, which can further support downstream tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Experiments</h2>
      <!-- <div class="content has-text-justified">
        <p>
          The videos in the top-left, top-right, bottom-left, and bottom-right corners represent: the original video, the original video with the yellow mask rendered using the camera extrinsics estimated by CalibAll, the additional depth map of the robot arm, and the additional joint masks of the robot arm, respectively.
        </p>
      </div> -->
      <figure class="teaser-image">
        <img src="static/images/main_tab.jpg" alt="MainTab" loading="lazy" style="width:100%; height:auto; border-radius:6px;"/>
        <figcaption class="has-text-centered" style="margin-top:8px; color:#4a5568;"><strong>Quantitative result on DREAM and comparison with baselines.</strong> CalibAll achieves the highest performance on the AUC and ADD.</figcaption>
      </figure>

        <figure class="teaser-image">
        <img src="static/images/stage1.jpg" alt="Stage1" loading="lazy" style="width:100%; height:auto; border-radius:6px;"/>
        <figcaption class="has-text-centered" style="margin-top:8px; color:#4a5568;"><strong>Qualitative result of EEF recognition on Franka, xArm and UR5e.</strong> 
The first row shows the heatmaps obtained from feature matching.
The second row visualizes the selected tracking point based on the maximum similarity.</figcaption>
      </figure>
      <figure class="teaser-image">
        <img src="static/images/stage2.jpg" alt="Stage2" loading="lazy" style="width:100%; height:auto; border-radius:6px;"/>
        <figcaption class="has-text-centered" style="margin-top:8px; color:#4a5568;"><strong>Qualitative result of coarse initialization and extrinsic refinement on Franka, xArm and UR5e.</strong> The first row presents the source RGB images. The second row shows the rendered result using the camera extrinsic obtained from the automatic coarse initialization approach. The last row shows the rendered result of final camera extrinsic produced by CalibAll.</figcaption>
      </figure>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Rendering Result on Different Robot</h2>
      <div class="content has-text-justified">
        <!-- TODO: Replace with your paper abstract -->
        <p>
          The videos in the top-left, top-right, bottom-left, and bottom-right corners represent: the original video, the original video with the yellow mask rendered using the camera extrinsics estimated by CalibAll, the additional depth map of the robot arm, and the additional joint masks of the robot arm, respectively.
        </p>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/franka+hand.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/franka+robotiq.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/xarm.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video4" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/ur5.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">More Rendering Result on DROID</h2>
      <div class="content has-text-justified">
        <!-- TODO: Replace with your paper abstract -->
        <p>
          The videos in the top-left, top-right, bottom-left, and bottom-right corners represent: the original video, the original video with the yellow mask rendered using the camera extrinsics estimated by CalibAll, the additional depth map of the robot arm, and the additional joint masks of the robot arm, respectively.
        </p>
      </div>
      <div class="carousel results-carousel">
        <div class="item item-video1">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/0.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/4.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

























<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{YourPaperKey2024,
  title={Your Paper Title Here},
  author={First Author and Second Author and Third Author},
  journal={Conference/Journal Name},
  year={2024},
  url={https://your-domain.com/your-project-page}
}</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <!-- <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div> -->
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
